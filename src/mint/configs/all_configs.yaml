mint:
  esm2:
    encoder_layers: 33
    encoder_embed_dim: 1280
    encoder_attention_heads: 16
    token_dropout: 0.1
    use_multimer: true
  
train:
  freeze_self_attn: false
  optim:
    lr: 1e-4
    adam_betas: "[0.9, 0.98]"
    adam_eps: 1e-8
    weight_decay: 0.01
    warmup_updates: 2000
    end_learning_rate: 1e-5
    total_num_update: 100000
