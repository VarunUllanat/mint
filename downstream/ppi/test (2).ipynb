{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6fb5725-0c5d-47a7-9736-214df755d04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,re\n",
    "import argparse, json\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "#from tqdm.notebook import tqdm\n",
    "from Bio.PDB.PDBParser import PDBParser\n",
    "from Bio.PDB.Polypeptide import one_to_index\n",
    "from Bio.PDB import Selection\n",
    "from Bio import SeqIO\n",
    "from Bio.PDB.Residue import Residue\n",
    "from easydict import EasyDict\n",
    "import enum\n",
    "sys.path.append('/data/cb/scratch/varun/esm-multimer/esm-multimer/')\n",
    "import esm, gzip\n",
    "from Bio import SeqIO\n",
    "from esm.model.esm2 import ESM2\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d69a5e36-9dbd-4193-9bdd-ad2c7bd64bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.load(f'embeddings/nofreeze_filtered50_70000_Intra2_embeddings.pt').numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ab63b27-2d65-494a-a686-35331785db4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52048, 2560)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89376a59-4550-43b5-8964-8c7aa1796882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "417328640"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "163019*2560 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe09fb1-b573-496b-83b2-4eaecf8f0c3a",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc243e1c-7cda-4c2e-8bc4-9372e4a6baa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59260 out of 59260 data points found\n",
      "163019 out of 163192 data points found\n",
      "52048 out of 52048 data points found\n"
     ]
    }
   ],
   "source": [
    "fasta_dictionary = SeqIO.to_dict(SeqIO.parse('human_swissprot_oneliner.fasta', \"fasta\"))\n",
    "\n",
    "def make_new_dfs(data_name):\n",
    "    data_df_neg = pd.read_csv(f'{data_name}_neg_rr.txt', sep=' ', header=None)\n",
    "    data_df_pos = pd.read_csv(f'{data_name}_pos_rr.txt', sep=' ', header=None)\n",
    "    \n",
    "    data_df = pd.concat([data_df_neg, data_df_pos], ignore_index=True)\n",
    "    labels = [0]*len(data_df_neg) + [1]*len(data_df_pos)\n",
    "    \n",
    "    seq1 = []\n",
    "    seq2 = []\n",
    "    new_labels = []\n",
    "    for index, row in data_df.iterrows():\n",
    "        if row[0] in fasta_dictionary and row[1] in fasta_dictionary:\n",
    "            seq1.append(str(fasta_dictionary[row[0]].seq))\n",
    "            seq2.append(str(fasta_dictionary[row[1]].seq))\n",
    "            new_labels.append(labels[index])\n",
    "    seq_df = pd.DataFrame({'seq1': seq1, 'seq2': seq2, 'labels':new_labels})\n",
    "    seq_df.to_csv(f'{data_name}_seqs.csv', index=False)\n",
    "    print(f'{len(seq_df)} out of {len(data_df)} data points found')\n",
    "\n",
    "make_new_dfs('Intra0')\n",
    "make_new_dfs('Intra1')\n",
    "make_new_dfs('Intra2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b35aa28-8e92-4c1c-adc4-99d66decdc21",
   "metadata": {},
   "source": [
    "## Get embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e9e842a-bdf9-47b0-96b5-183d5b2df13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPIDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        data_name, \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_df = pd.read_csv(f'{data_name}_seqs.csv')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data_df.iloc[index]\n",
    "        s0, s1 = row[\"seq1\"], row[\"seq2\"]\n",
    "        label = int(row[\"labels\"])\n",
    "        \n",
    "        return s0, s1, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e6ee0c9-dc26-43eb-98f0-183368dc3f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPICollateFn:\n",
    "    \n",
    "    def __init__(self, truncation_seq_length=None):\n",
    "        self.alphabet = esm.data.Alphabet.from_architecture(\"ESM-1b\")\n",
    "        self.truncation_seq_length = truncation_seq_length\n",
    "\n",
    "    def __call__(self, batches):\n",
    "        batch_size = len(batches)\n",
    "        heavy_chain, light_chain, labels = zip(*batches)\n",
    "        \n",
    "        chains = [self.convert(c) for c in [heavy_chain, light_chain]]\n",
    "        chain_ids = [torch.ones(c.shape, dtype=torch.int32) * i for i, c in enumerate(chains)]\n",
    "        chains = torch.cat(chains, -1)\n",
    "        chain_ids = torch.cat(chain_ids, -1)\n",
    "        labels = torch.from_numpy(np.stack(labels, 0))\n",
    "        \n",
    "        return chains, chain_ids, labels\n",
    "\n",
    "    def convert(self, seq_str_list):\n",
    "        batch_size = len(seq_str_list)\n",
    "        seq_encoded_list = [self.alphabet.encode('<cls>' + seq_str.replace('J', 'L') + '<eos>') for seq_str in seq_str_list]\n",
    "        if self.truncation_seq_length:\n",
    "            for i in range(batch_size):\n",
    "                seq = seq_encoded_list[i]\n",
    "                if len(seq) > self.truncation_seq_length:\n",
    "                    start = random.randint(0, len(seq) - self.truncation_seq_length + 1)\n",
    "                    seq_encoded_list[i] = seq[start:start+self.truncation_seq_length]\n",
    "        max_len = max(len(seq_encoded) for seq_encoded in seq_encoded_list)\n",
    "        if self.truncation_seq_length:\n",
    "            assert max_len <= self.truncation_seq_length\n",
    "        tokens = torch.empty((batch_size, max_len), dtype=torch.int64)\n",
    "        tokens.fill_(self.alphabet.padding_idx)\n",
    "        \n",
    "        for i, seq_encoded in enumerate(seq_encoded_list):\n",
    "            seq = torch.tensor(seq_encoded, dtype=torch.int64)\n",
    "            tokens[i,:len(seq_encoded)] = seq\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b9d4427-7281-4a26-8d87-10f439246b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upgrade_state_dict(state_dict):\n",
    "    \"\"\"Removes prefixes 'model.encoder.sentence_encoder.' and 'model.encoder.'.\"\"\"\n",
    "    prefixes = [\"encoder.sentence_encoder.\", \"encoder.\"]\n",
    "    pattern = re.compile(\"^\" + \"|\".join(prefixes))\n",
    "    state_dict = {pattern.sub(\"\", name): param for name, param in state_dict.items()}\n",
    "    return state_dict\n",
    "\n",
    "class PPIWrapper(nn.Module):\n",
    "    def __init__(self, cfg, checkpoint_path, freeze_percent=0.0, use_multimer=True, device='cuda:0'):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.model = ESM2(\n",
    "            num_layers=cfg.encoder_layers,\n",
    "            embed_dim=cfg.encoder_embed_dim,\n",
    "            attention_heads=cfg.encoder_attention_heads,\n",
    "            token_dropout=cfg.token_dropout,\n",
    "            use_multimer = use_multimer,\n",
    "        )\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "        if use_multimer:\n",
    "            # remove 'model.' in keys\n",
    "            new_checkpoint = OrderedDict((key.replace('model.', ''), value) for key, value in checkpoint['state_dict'].items())\n",
    "            self.model.load_state_dict(new_checkpoint)\n",
    "        else:\n",
    "            new_checkpoint = upgrade_state_dict(checkpoint['model'])\n",
    "            self.model.load_state_dict(new_checkpoint)\n",
    "        total_layers = 33\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if 'embed_tokens.weight' in name or '_norm_after' in name or 'lm_head' in name:\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                layer_num = name.split('.')[1]\n",
    "                if int(layer_num) <= math.floor(total_layers*freeze_percent):\n",
    "                    param.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, chains, chain_ids):\n",
    "        mask = (~chains.eq(self.model.cls_idx)) & (~chains.eq(self.model.eos_idx)) & (~chains.eq(self.model.padding_idx))\n",
    "        chain_out = self.model(chains, chain_ids, repr_layers=[33])[\"representations\"][33]\n",
    "        mask_expanded = mask.unsqueeze(-1).expand_as(chain_out)\n",
    "        masked_chain_out = chain_out * mask_expanded\n",
    "        sum_masked = masked_chain_out.sum(dim=1)\n",
    "        mask_counts = mask.sum(dim=1, keepdim=True).float()  # Convert to float for division\n",
    "        mean_chain_out = sum_masked / mask_counts\n",
    "        return mean_chain_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9f6f5d1-00c8-4939-a8e5-5b8a2f3af820",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader, name):\n",
    "\n",
    "    device = 'cuda:7'\n",
    "    model.to(device)\n",
    "\n",
    "    full_embs = []\n",
    "\n",
    "    for step, eval_batch in enumerate(tqdm(loader)):\n",
    "        \n",
    "        chains, chain_ids, target = eval_batch\n",
    "        chains = chains.to(device)\n",
    "        chain_ids = chain_ids.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        embs = model(chains, chain_ids)\n",
    "\n",
    "        full_embs.append(embs.squeeze(-1).detach().cpu())\n",
    "\n",
    "    full_embs = torch.cat(full_embs).ravel()\n",
    "    torch.save(full_embs, f'./embeddings/{name}_embeddings.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7429e1e3-9e07-4a7e-919e-3c502f97fc12",
   "metadata": {},
   "source": [
    "## Train MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5395f9d-fcee-4c58-a2dd-8ca0b47fb923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_curve, auc, f1_score, precision_score, recall_score, confusion_matrix, matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "396d6426-9952-4354-a42f-8e90856f6051",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPI_MLP_Dataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        x, y\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_layers, hidden_size, dropout, activation):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        self.layers.append(nn.Linear(input_size, hidden_size))\n",
    "        \n",
    "        for _ in range(1, num_layers):\n",
    "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "        \n",
    "        self.layers.append(nn.Linear(hidden_size, output_size))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        if activation == \"relu\": self.activation = torch.nn.ReLU()\n",
    "        elif activation == \"silu\": self.activation = torch.nn.SiLU()\n",
    "        elif activation == \"identity\": self.activation = torch.nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply layers with activation and dropout\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = layer(x)\n",
    "            x = self.activation(x)\n",
    "            x = self.dropout(x)\n",
    "        # Output layer, no activation and dropout\n",
    "        x = self.layers[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2c72c10-02cf-4037-bcd4-d98796a90e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = PPIDataset('Intra1').data_df['labels'].tolist()\n",
    "y_val = PPIDataset('Intra0').data_df['labels'].tolist()\n",
    "y_test = PPIDataset('Intra2').data_df['labels'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf295887-8a7d-4cd0-b675-200a72f73379",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.load('embeddings/nofreeze_Intra1_embeddings.pt').numpy().reshape(len(y_train), -1)\n",
    "X_valid = torch.load('embeddings/nofreeze_Intra0_embeddings.pt').numpy().reshape(len(y_val), -1)\n",
    "X_test = torch.load('embeddings/nofreeze_Intra2_embeddings.pt').numpy().reshape(len(y_test), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7560d45-2767-4c46-8c4b-6231e559772c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_train_test(X_train, X_valid, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_valid = scaler.transform(X_valid)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    return X_train, X_valid, X_test\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train_norm, X_valid_norm, X_test_norm = normalize_train_test(X_train, X_valid, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe6d7dd2-aac6-4ee3-948b-58d4444929ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "clf = Ridge(alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a047b70f-d033-44d5-a64e-97a02cee8942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=0.1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train_norm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c61d802f-01de-4c6c-ab76-2e93e7511e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0684549-1442-4835-ae3b-67b6fd69986d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 0.5022287119581924,\n",
       " 'AUPRC': 0.5190343066629095,\n",
       " 'F1 Score': 0.023003243080171962,\n",
       " 'Precision': 0.6174089068825911,\n",
       " 'Recall': 0.011719950814632648,\n",
       " 'Specificity': 0.9927374731017522,\n",
       " 'MCC': 0.02298599623136244}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_metrics(y_test, preds, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30f732f7-e612-41b3-85ae-7cb9515cddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_metrics(targets, predictions, threshold=0.5):\n",
    "    # Convert probabilities to binary predictions based on a threshold\n",
    "    binary_predictions = (predictions >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(targets, binary_predictions)\n",
    "    \n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = precision_score(targets, binary_predictions)\n",
    "    recall = recall_score(targets, binary_predictions)\n",
    "    f1 = f1_score(targets, binary_predictions)\n",
    "    mcc = matthews_corrcoef(targets, binary_predictions)\n",
    "    \n",
    "    # Calculate specificity (True Negative Rate)\n",
    "    tn, fp, fn, tp = confusion_matrix(targets, binary_predictions).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    \n",
    "    # Calculate AUPRC\n",
    "    precision_vals, recall_vals, _ = precision_recall_curve(targets, predictions)\n",
    "    auprc = auc(recall_vals, precision_vals)\n",
    "\n",
    "    \n",
    "    return {\n",
    "        'Accuracy': accuracy,\n",
    "        'AUPRC': auprc,\n",
    "        'F1 Score': f1,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'Specificity': specificity,\n",
    "        'MCC': mcc\n",
    "    }\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device='cuda'):\n",
    "\n",
    "    preds = []\n",
    "    targets = []\n",
    "\n",
    "    for step, eval_batch in enumerate(tqdm(loader)):\n",
    "        embs, target = eval_batch\n",
    "        embs = embs.to(device)\n",
    "        target = target.to(device)\n",
    "        pred = model(embs).squeeze(-1)  \n",
    "\n",
    "        pred = torch.sigmoid(pred)\n",
    "\n",
    "        preds.append(pred.detach().cpu().numpy())\n",
    "        targets.append(target.cpu().numpy())\n",
    "\n",
    "    preds = np.concatenate(preds)\n",
    "    targets = np.concatenate(targets)\n",
    "\n",
    "    metrics_dict = classification_metrics(targets, preds)\n",
    "    return metrics_dict\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, num_epochs, device='cuda'):\n",
    "    optimizer = torch.optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, model.parameters()), \n",
    "            lr=1e-3)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Training at epoch {epoch}')\n",
    "        loss_accum = 0\n",
    "        for step, train_batch in enumerate(tqdm(train_loader)):\n",
    "\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            embs, target = train_batch\n",
    "            embs = embs.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            pred = model(embs).squeeze(-1)   \n",
    "            loss = loss_fn(pred, target.float())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_accum += loss.detach().cpu().item()\n",
    "        print(f'Loss at end of epoch {epoch}: {loss_accum/(step+1)}')\n",
    "\n",
    "        print(f'Evaluating at epoch {epoch}')\n",
    "        metrics_dict = evaluate(model, val_loader)\n",
    "        print(metrics_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "21a37c85-dfb9-43e8-9fad-2dcb4a894733",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PPI_MLP_Dataset(X_train, y_train)\n",
    "val_dataset = PPI_MLP_Dataset(X_valid, y_val)\n",
    "test_dataset = PPI_MLP_Dataset(X_test, y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=512, shuffle=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=512, shuffle=False\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=512, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cd20ccac-d4d6-4632-a2aa-2cc57e731d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  SimpleMLP(1280, 1, 2, 128, 0.5, \"relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "80b823bb-f63f-4499-b150-79cc9f0a89b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 319/319 [00:01<00:00, 160.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at end of epoch 0: 0.6926425403935782\n",
      "Evaluating at epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 102/102 [00:00<00:00, 318.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.5256494005533354, 'AUPRC': 0.535426084216603, 'F1 Score': 0.45161147020279424, 'Precision': 0.535137126914776, 'Recall': 0.39063940977559175, 'Specificity': 0.660659391331079, 'MCC': 0.05327782073846981}\n",
      "Training at epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 319/319 [00:01<00:00, 172.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at end of epoch 1: 0.6895941010089504\n",
      "Training at epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 319/319 [00:01<00:00, 180.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at end of epoch 2: 0.6856047546601968\n",
      "Training at epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 319/319 [00:01<00:00, 180.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at end of epoch 3: 0.6827683809408948\n",
      "Training at epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 319/319 [00:01<00:00, 186.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at end of epoch 4: 0.6804738140031462\n",
      "Training at epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 319/319 [00:01<00:00, 182.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at end of epoch 5: 0.6783258232950791\n",
      "Evaluating at epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 102/102 [00:00<00:00, 332.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.5773708884106978, 'AUPRC': 0.6140395752187259, 'F1 Score': 0.5562526477174154, 'Precision': 0.5855098314010277, 'Recall': 0.5297802028896403, 'Specificity': 0.6249615739317553, 'MCC': 0.15544751674512236}\n",
      "Training at epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 319/319 [00:01<00:00, 176.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at end of epoch 6: 0.6758756245191568\n",
      "Training at epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 319/319 [00:01<00:00, 180.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at end of epoch 7: 0.6752051077666328\n",
      "Training at epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 319/319 [00:01<00:00, 167.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at end of epoch 8: 0.6732667721924737\n",
      "Training at epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 319/319 [00:01<00:00, 187.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at end of epoch 9: 0.6725723214657703\n",
      "Training at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 319/319 [00:01<00:00, 186.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at end of epoch 10: 0.6717646546124665\n",
      "Evaluating at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 102/102 [00:00<00:00, 242.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.587573009529665, 'AUPRC': 0.624404507589362, 'F1 Score': 0.5518206113245364, 'Precision': 0.604197147037308, 'Recall': 0.5078004918536735, 'Specificity': 0.6673455272056563, 'MCC': 0.17741863626585555}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, test_loader, num_epochs=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5e8604-3407-4fda-ba99-6c1c8dc7f59b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d615ae5c-33cf-4ef7-aa0b-7711aa520970",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86070938-51a0-4c84-8792-ea7c2be9838a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03457c5-47cf-447a-8df3-f01971362b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b608187-f902-468e-bc00-d5fd86d64fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6cd5cd-8820-40f1-a6c8-94aa5e7c618e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "109b7f55-a305-4349-ba51-0c5c95c8ec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"activation\": [\"logistic\", \"relu\", \"identity\"],\n",
    "    \"alpha\": [0.0001, 0.001, 0.01],\n",
    "    \"learning_rate\": [\"adaptive\"],\n",
    "    \"solver\": [\"adam\"],\n",
    "    \"learning_rate_init\": [0.001, 0.01],\n",
    "    \"max_iter\": [1000, 2000],\n",
    "    \"hidden_layer_sizes\": [\n",
    "        (64,), (128,), (512,),\n",
    "        (64, 64), (128, 128),\n",
    "        (64, 64, 64),\n",
    "        ],\n",
    "    \"early_stopping\": [True],\n",
    "    \"validation_fraction\": [0.1],\n",
    "    \"tol\": [1e-4, 1e-5],\n",
    "}\n",
    "\n",
    "param_grid = {\"max_iter\": [1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cecc773f-05ff-42e1-8a61-4cfceea009e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_index = np.concatenate([-1 * np.ones(X_train_norm.shape[0]), np.zeros(X_valid_norm.shape[0])], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "62a6113f-1e6e-44b0-b66d-413b3fab5bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = PredefinedSplit(valid_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f64d65ea-71fd-4f7a-a600-13ba1c6e5aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0d58d4b7-75d9-4f98-8113-cfa505021858",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = [\"accuracy\", \"average_precision\", \"f1\", \"recall\", \"precision\", \"roc_auc\"]\n",
    "refit = \"average_precision\"\n",
    "verbose = 10\n",
    "n_jobs = -1\n",
    "\n",
    "clsf = GridSearchCV(mlp, param_grid, \n",
    "                    cv=cv, \n",
    "                    scoring=scoring, \n",
    "                    verbose=verbose, \n",
    "                    n_jobs=n_jobs, \n",
    "                    refit=refit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bac3e1f0-6da7-4138-b4b5-90ddcc77a346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/cb/scratch/varun/miniconda3/envs/esmfold/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "             estimator=MLPClassifier(), n_jobs=-1, param_grid={'max_iter': [1]},\n",
       "             refit='average_precision',\n",
       "             scoring=['accuracy', 'average_precision', 'f1', 'recall',\n",
       "                      'precision', 'roc_auc'],\n",
       "             verbose=10)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clsf.fit(X_trainval_norm, y_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b22e1bab-7c5a-4fd9-9354-622906f56aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator = clsf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c6750d90-0797-47d3-85e2-e486c7580d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/1; 1/1] START max_iter=1..................................................\n",
      "[CV 1/1; 1/1] END max_iter=1; accuracy: (test=0.537) average_precision: (test=0.555) f1: (test=0.528) precision: (test=0.539) recall: (test=0.518) roc_auc: (test=0.554) total time=   3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/cb/scratch/varun/miniconda3/envs/esmfold/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  ConvergenceWarning,\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = best_estimator.predict_proba(X_test_norm)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed92de1f-8d91-4818-baad-1e07acacb49b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esmfold",
   "language": "python",
   "name": "esmfold"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
